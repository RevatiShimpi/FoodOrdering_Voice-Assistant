{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (1.23.5)\n",
      "Collecting tokenizers==0.8.0-rc4 (from transformers==3)\n",
      "  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (3.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (4.66.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (2022.10.31)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (0.1.99)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (0.0.53)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers==3) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->transformers==3) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacremoses->transformers==3) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacremoses->transformers==3) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacremoses->transformers==3) (1.2.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [48 lines of output]\n",
      "      C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-vtnqm1bs\\overlay\\Lib\\site-packages\\setuptools\\dist.py:314: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n",
      "        self.metadata.version = self._normalize_version(\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What would you like to order in Barbecue?</td>\n",
       "      <td>food_order.type.food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, I'd like to get a rack of ribs.</td>\n",
       "      <td>food_order.name.item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With rack of ribs, I would suggest Barbecue sa...</td>\n",
       "      <td>food_order.name.item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With rack of ribs, I would suggest Barbecue sa...</td>\n",
       "      <td>food_order.name.item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah. Of course, I want the barbecue sauce and...</td>\n",
       "      <td>food_order.name.item</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence                 Label\n",
       "0          What would you like to order in Barbecue?  food_order.type.food\n",
       "1              Yeah, I'd like to get a rack of ribs.  food_order.name.item\n",
       "2  With rack of ribs, I would suggest Barbecue sa...  food_order.name.item\n",
       "3  With rack of ribs, I would suggest Barbecue sa...  food_order.name.item\n",
       "4  Yeah. Of course, I want the barbecue sauce and...  food_order.name.item"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Replace smart quotes with regular double quotes\n",
    "df = pd.read_excel(\"bert_nlp_data_main1.xlsx\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "food_order.name.item                 6022\n",
       "food_order.other_description.item    1453\n",
       "food_order.type.food                 1224\n",
       "food_order.time.pickup               1045\n",
       "food_order.num.people                 856\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    0.568113\n",
       "2    0.137075\n",
       "4    0.115472\n",
       "3    0.098585\n",
       "1    0.080755\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the labels into encodings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Label'] = le.fit_transform(df['Label'])\n",
    "# check class distribution\n",
    "df['Label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_labels = df['Sentence'], df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# Load the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Import the DistilBert pretrained model\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1037,  4487, 16643,  2140, 14324,  2944,  1012,\n",
      "           102],\n",
      "        [  101,  2951,  2003,  3514,   102,     0,     0,     0,     0,     0,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In input_ids:\\n101 - Indicates beginning of the sentence\\n102 - Indicates end of the sentence\\nIn attention_mask:\\n1 - Actual token\\n0 - Padded token'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"this is a distil bert model.\",\"data is oil\"]\n",
    "# Encode the text\n",
    "encoded_input = tokenizer(text, padding=True,truncation=True, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "'''In input_ids:\n",
    "101 - Indicates beginning of the sentence\n",
    "102 - Indicates end of the sentence\n",
    "In attention_mask:\n",
    "1 - Actual token\n",
    "0 - Padded token'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApBUlEQVR4nO3df3RUdX7/8Vd+TggwieBmQgrBtGyFCMiPLGRW1wKGRDa1KmnPallMFfVAgzXkFJSKLD/WhrJFxDVKd0Viz0oRetRdfqzJGASWEn5licuPXdat2FBhklYahp+TIXO/f+w3dxkJkAlhhk98Ps6Zk9zPfd9PPvftJLy8MzeJsSzLEgAAgEFio70AAACAcBFgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGiY/2Am6UYDCo48ePq3fv3oqJiYn2cgAAQAdYlqXTp08rIyNDsbFXvs7SbQPM8ePHNWDAgGgvAwAAdMKxY8fUv3//K+7vtgGmd+/ekn7fAKfT2el5AoGAqqurlZ+fr4SEhK5aHr6EPkcGfY4M+hw59DoyItlnn8+nAQMG2P+OX0m3DTBtLxs5nc7rDjDJyclyOp18c9xA9Dky6HNk0OfIodeREY0+X+vtH7yJFwAAGCesALNgwQLFxMSEPAYPHmzvv3DhgkpKStS3b1/16tVLRUVFamxsDJmjoaFBhYWFSk5OVlpammbPnq2LFy+G1GzdulWjRo2Sw+HQoEGDVFlZ2fkzBAAA3U7YV2DuuOMOnThxwn7s2LHD3jdr1ixt2LBB69ev17Zt23T8+HFNnjzZ3t/a2qrCwkK1tLRo586deuutt1RZWan58+fbNUePHlVhYaHGjx+v+vp6lZaW6oknnlBVVdV1nioAAOguwn4PTHx8vNLT0y8bP3XqlFatWqU1a9ZowoQJkqTVq1dryJAh2rVrl3Jzc1VdXa3Dhw/rww8/lMvl0ogRI7R48WI9++yzWrBggRITE7Vy5UplZWVp2bJlkqQhQ4Zox44dWr58uQoKCq7zdAEAQHcQdoD55JNPlJGRoaSkJLndbpWXlyszM1N1dXUKBALKy8uzawcPHqzMzEzV1tYqNzdXtbW1GjZsmFwul11TUFCgGTNm6NChQxo5cqRqa2tD5mirKS0tveq6/H6//H6/ve3z+ST9/o1HgUAg3NO0tR17PXPg2uhzZNDnyKDPkUOvIyOSfe7o1wgrwIwdO1aVlZW6/fbbdeLECS1cuFDf+ta3dPDgQXm9XiUmJio1NTXkGJfLJa/XK0nyer0h4aVtf9u+q9X4fD6dP39ePXr0aHdt5eXlWrhw4WXj1dXVSk5ODuc02+XxeK57DlwbfY4M+hwZ9Dly6HVkRKLP586d61BdWAFm0qRJ9ufDhw/X2LFjNXDgQK1bt+6KwSJS5s6dq7KyMnu77T7y/Pz8676N2uPxaOLEidyidwPR58igz5FBnyOHXkdGJPvc9grKtVzX74FJTU3Vn/7pn+p3v/udJk6cqJaWFjU3N4dchWlsbLTfM5Oenq49e/aEzNF2l9KlNV++c6mxsVFOp/OqIcnhcMjhcFw2npCQ0CXN7qp5cHX0OTLoc2TQ58ih15ERiT53dP7r+j0wZ86c0X/+53+qX79+Gj16tBISElRTU2PvP3LkiBoaGuR2uyVJbrdbBw4cUFNTk13j8XjkdDqVnZ1t11w6R1tN2xwAAABhBZi///u/17Zt2/TZZ59p586deuihhxQXF6dHHnlEKSkpmjZtmsrKyvTRRx+prq5Ojz32mNxut3JzcyVJ+fn5ys7O1tSpU/Xxxx+rqqpK8+bNU0lJiX31ZPr06fr00081Z84c/eY3v9Frr72mdevWadasWV1/9gAAwEhhvYT03//933rkkUf0xRdf6Gtf+5ruvvtu7dq1S1/72tckScuXL1dsbKyKiork9/tVUFCg1157zT4+Li5OGzdu1IwZM+R2u9WzZ08VFxdr0aJFdk1WVpY2bdqkWbNmacWKFerfv7/eeOMNbqEGAAC2sALM2rVrr7o/KSlJFRUVqqiouGLNwIEDtXnz5qvOM27cOO3fvz+cpQEAgK8Q/hYSAAAwDgEGAAAY57puo4Y5bntuU7SXcFWOOEtLx0hDF1TJ3/qHP6H+2ZLCKK4KAHCz4goMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABjnugLMkiVLFBMTo9LSUnvswoULKikpUd++fdWrVy8VFRWpsbEx5LiGhgYVFhYqOTlZaWlpmj17ti5evBhSs3XrVo0aNUoOh0ODBg1SZWXl9SwVAAB0I50OMHv37tW//Mu/aPjw4SHjs2bN0oYNG7R+/Xpt27ZNx48f1+TJk+39ra2tKiwsVEtLi3bu3Km33npLlZWVmj9/vl1z9OhRFRYWavz48aqvr1dpaameeOIJVVVVdXa5AACgG+lUgDlz5oymTJmiH//4x7rlllvs8VOnTmnVqlV66aWXNGHCBI0ePVqrV6/Wzp07tWvXLklSdXW1Dh8+rJ/85CcaMWKEJk2apMWLF6uiokItLS2SpJUrVyorK0vLli3TkCFDNHPmTP3lX/6lli9f3gWnDAAATBffmYNKSkpUWFiovLw8ff/737fH6+rqFAgElJeXZ48NHjxYmZmZqq2tVW5urmprazVs2DC5XC67pqCgQDNmzNChQ4c0cuRI1dbWhszRVnPpS1Vf5vf75ff77W2fzydJCgQCCgQCnTlN+/hLP5rKEWdFewlX5Yi1Qj62Mb3vN5vu8ny+2dHnyKHXkRHJPnf0a4QdYNauXatf/vKX2rt372X7vF6vEhMTlZqaGjLucrnk9XrtmkvDS9v+tn1Xq/H5fDp//rx69Ohx2dcuLy/XwoULLxuvrq5WcnJyx0/wCjwez3XPEU1Lx0R7BR2zOCcYsr158+YoraR7M/35bAr6HDn0OjIi0edz5851qC6sAHPs2DE988wz8ng8SkpK6tTCbpS5c+eqrKzM3vb5fBowYIDy8/PldDo7PW8gEJDH49HEiROVkJDQFUuNiqELbu73DzliLS3OCeqFfbHyB2Ps8YMLCqK4qu6nuzyfb3b0OXLodWREss9tr6BcS1gBpq6uTk1NTRo1apQ91traqu3bt+vVV19VVVWVWlpa1NzcHHIVprGxUenp6ZKk9PR07dmzJ2TetruULq358p1LjY2Ncjqd7V59kSSHwyGHw3HZeEJCQpc0u6vmiRZ/a8y1i24C/mBMyFpN7vnNzPTnsynoc+TQ68iIRJ87On9Yb+K99957deDAAdXX19uPnJwcTZkyxf48ISFBNTU19jFHjhxRQ0OD3G63JMntduvAgQNqamqyazwej5xOp7Kzs+2aS+doq2mbAwAAfLWFdQWmd+/eGjp0aMhYz5491bdvX3t82rRpKisrU58+feR0OvX000/L7XYrNzdXkpSfn6/s7GxNnTpVS5culdfr1bx581RSUmJfQZk+fbpeffVVzZkzR48//ri2bNmidevWadOmTV1xzgAAwHCdugvpapYvX67Y2FgVFRXJ7/eroKBAr732mr0/Li5OGzdu1IwZM+R2u9WzZ08VFxdr0aJFdk1WVpY2bdqkWbNmacWKFerfv7/eeOMNFRTwfggAANAFAWbr1q0h20lJSaqoqFBFRcUVjxk4cOA17y4ZN26c9u/ff73LAwAA3RB/CwkAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA48RHewEmuu25TdFeAgAAX2lhXYF5/fXXNXz4cDmdTjmdTrndbv385z+391+4cEElJSXq27evevXqpaKiIjU2NobM0dDQoMLCQiUnJystLU2zZ8/WxYsXQ2q2bt2qUaNGyeFwaNCgQaqsrOz8GQIAgG4nrADTv39/LVmyRHV1ddq3b58mTJigBx54QIcOHZIkzZo1Sxs2bND69eu1bds2HT9+XJMnT7aPb21tVWFhoVpaWrRz50699dZbqqys1Pz58+2ao0ePqrCwUOPHj1d9fb1KS0v1xBNPqKqqqotOGQAAmC6sl5Duv//+kO0XX3xRr7/+unbt2qX+/ftr1apVWrNmjSZMmCBJWr16tYYMGaJdu3YpNzdX1dXVOnz4sD788EO5XC6NGDFCixcv1rPPPqsFCxYoMTFRK1euVFZWlpYtWyZJGjJkiHbs2KHly5eroKCgi04bAACYrNPvgWltbdX69et19uxZud1u1dXVKRAIKC8vz64ZPHiwMjMzVVtbq9zcXNXW1mrYsGFyuVx2TUFBgWbMmKFDhw5p5MiRqq2tDZmjraa0tPSq6/H7/fL7/fa2z+eTJAUCAQUCgc6epn3spXM44qxOz4f2OWKtkI9true/HS7X3vMZXY8+Rw69joxI9rmjXyPsAHPgwAG53W5duHBBvXr10nvvvafs7GzV19crMTFRqampIfUul0ter1eS5PV6Q8JL2/62fVer8fl8On/+vHr06NHuusrLy7Vw4cLLxqurq5WcnBzuaV7G4/HYny8dc93T4QoW5wRDtjdv3hyllXRvlz6fcePQ58ih15ERiT6fO3euQ3VhB5jbb79d9fX1OnXqlP793/9dxcXF2rZtW9gL7Gpz585VWVmZve3z+TRgwADl5+fL6XR2et5AICCPx6OJEycqISFBkjR0Ae/H6WqOWEuLc4J6YV+s/MEYe/zgAl427ErtPZ/R9ehz5NDryIhkn9teQbmWsANMYmKiBg0aJEkaPXq09u7dqxUrVug73/mOWlpa1NzcHHIVprGxUenp6ZKk9PR07dmzJ2S+truULq358p1LjY2NcjqdV7z6IkkOh0MOh+Oy8YSEhC5p9qXz+FtjrlGNzvIHY0L6yw+kG6Orvi9wdfQ5cuh1ZESizx2d/7p/kV0wGJTf79fo0aOVkJCgmpoae9+RI0fU0NAgt9stSXK73Tpw4ICamprsGo/HI6fTqezsbLvm0jnaatrmAAAACOsKzNy5czVp0iRlZmbq9OnTWrNmjbZu3aqqqiqlpKRo2rRpKisrU58+feR0OvX000/L7XYrNzdXkpSfn6/s7GxNnTpVS5culdfr1bx581RSUmJfPZk+fbpeffVVzZkzR48//ri2bNmidevWadMmfnkcAAD4vbACTFNTkx599FGdOHFCKSkpGj58uKqqqjRx4kRJ0vLlyxUbG6uioiL5/X4VFBTotddes4+Pi4vTxo0bNWPGDLndbvXs2VPFxcVatGiRXZOVlaVNmzZp1qxZWrFihfr376833niDW6gBAIAtrACzatWqq+5PSkpSRUWFKioqrlgzcODAa95ZMm7cOO3fvz+cpQEAgK8Q/pgjAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHHCCjDl5eX6xje+od69eystLU0PPvigjhw5ElJz4cIFlZSUqG/fvurVq5eKiorU2NgYUtPQ0KDCwkIlJycrLS1Ns2fP1sWLF0Nqtm7dqlGjRsnhcGjQoEGqrKzs3BkCAIBuJ6wAs23bNpWUlGjXrl3yeDwKBALKz8/X2bNn7ZpZs2Zpw4YNWr9+vbZt26bjx49r8uTJ9v7W1lYVFhaqpaVFO3fu1FtvvaXKykrNnz/frjl69KgKCws1fvx41dfXq7S0VE888YSqqqq64JQBAIDp4sMp/uCDD0K2KysrlZaWprq6Ot1zzz06deqUVq1apTVr1mjChAmSpNWrV2vIkCHatWuXcnNzVV1drcOHD+vDDz+Uy+XSiBEjtHjxYj377LNasGCBEhMTtXLlSmVlZWnZsmWSpCFDhmjHjh1avny5CgoKuujUAQCAqcIKMF926tQpSVKfPn0kSXV1dQoEAsrLy7NrBg8erMzMTNXW1io3N1e1tbUaNmyYXC6XXVNQUKAZM2bo0KFDGjlypGpra0PmaKspLS294lr8fr/8fr+97fP5JEmBQECBQKDT59h27KVzOOKsTs+H9jlirZCPba7nvx0u197zGV2PPkcOvY6MSPa5o1+j0wEmGAyqtLRUd911l4YOHSpJ8nq9SkxMVGpqakity+WS1+u1ay4NL2372/Zdrcbn8+n8+fPq0aPHZespLy/XwoULLxuvrq5WcnJy507yEh6Px/586Zjrng5XsDgnGLK9efPmKK2ke7v0+Ywbhz5HDr2OjEj0+dy5cx2q63SAKSkp0cGDB7Vjx47OTtGl5s6dq7KyMnvb5/NpwIABys/Pl9Pp7PS8gUBAHo9HEydOVEJCgiRp6ALei9PVHLGWFucE9cK+WPmDMfb4wQW8ZNiV2ns+o+vR58ih15ERyT63vYJyLZ0KMDNnztTGjRu1fft29e/f3x5PT09XS0uLmpubQ67CNDY2Kj093a7Zs2dPyHxtdyldWvPlO5caGxvldDrbvfoiSQ6HQw6H47LxhISELmn2pfP4W2OuUY3O8gdjQvrLD6Qbo6u+L3B19Dly6HVkRKLPHZ0/rLuQLMvSzJkz9d5772nLli3KysoK2T969GglJCSopqbGHjty5IgaGhrkdrslSW63WwcOHFBTU5Nd4/F45HQ6lZ2dbddcOkdbTdscAADgqy2sKzAlJSVas2aNfvrTn6p37972e1ZSUlLUo0cPpaSkaNq0aSorK1OfPn3kdDr19NNPy+12Kzc3V5KUn5+v7OxsTZ06VUuXLpXX69W8efNUUlJiX0GZPn26Xn31Vc2ZM0ePP/64tmzZonXr1mnTpk1dfPoAAMBEYV2Bef3113Xq1CmNGzdO/fr1sx/vvPOOXbN8+XL9+Z//uYqKinTPPfcoPT1d7777rr0/Li5OGzduVFxcnNxut7773e/q0Ucf1aJFi+yarKwsbdq0SR6PR3feeaeWLVumN954g1uoAQCApDCvwFjWtW8fTkpKUkVFhSoqKq5YM3DgwGveXTJu3Djt378/nOWhG7rtOfOuun22pDDaSwCAbo+/hQQAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOGEHmO3bt+v+++9XRkaGYmJi9P7774fstyxL8+fPV79+/dSjRw/l5eXpk08+Cak5efKkpkyZIqfTqdTUVE2bNk1nzpwJqfnVr36lb33rW0pKStKAAQO0dOnS8M8OAAB0S2EHmLNnz+rOO+9URUVFu/uXLl2qV155RStXrtTu3bvVs2dPFRQU6MKFC3bNlClTdOjQIXk8Hm3cuFHbt2/XU089Ze/3+XzKz8/XwIEDVVdXpx/84AdasGCBfvSjH3XiFAEAQHcTH+4BkyZN0qRJk9rdZ1mWXn75Zc2bN08PPPCAJOlf//Vf5XK59P777+vhhx/Wr3/9a33wwQfau3evcnJyJEk//OEP9e1vf1v//M//rIyMDL399ttqaWnRm2++qcTERN1xxx2qr6/XSy+9FBJ0AADAV1PYAeZqjh49Kq/Xq7y8PHssJSVFY8eOVW1trR5++GHV1tYqNTXVDi+SlJeXp9jYWO3evVsPPfSQamtrdc899ygxMdGuKSgo0D/90z/p//7v/3TLLbdc9rX9fr/8fr+97fP5JEmBQECBQKDT59R27KVzOOKsTs+H9jlirZCPJrue59uN1t7zGV2PPkcOvY6MSPa5o1+jSwOM1+uVJLlcrpBxl8tl7/N6vUpLSwtdRHy8+vTpE1KTlZV12Rxt+9oLMOXl5Vq4cOFl49XV1UpOTu7kGf2Bx+OxP1865rqnwxUszglGewnXbfPmzdFewjVd+nzGjUOfI4deR0Yk+nzu3LkO1XVpgImmuXPnqqyszN72+XwaMGCA8vPz5XQ6Oz1vIBCQx+PRxIkTlZCQIEkauqDquteLUI5YS4tzgnphX6z8wZhoL+e6HFxQEO0lXFF7z2d0PfocOfQ6MiLZ57ZXUK6lSwNMenq6JKmxsVH9+vWzxxsbGzVixAi7pqmpKeS4ixcv6uTJk/bx6enpamxsDKlp226r+TKHwyGHw3HZeEJCQpc0+9J5/K1m/wN7M/MHY4zvrwk/RLvq+wJXR58jh15HRiT63NH5u/T3wGRlZSk9PV01NTX2mM/n0+7du+V2uyVJbrdbzc3Nqqurs2u2bNmiYDCosWPH2jXbt28PeR3M4/Ho9ttvb/flIwAA8NUSdoA5c+aM6uvrVV9fL+n3b9ytr69XQ0ODYmJiVFpaqu9///v62c9+pgMHDujRRx9VRkaGHnzwQUnSkCFDdN999+nJJ5/Unj179B//8R+aOXOmHn74YWVkZEiS/vqv/1qJiYmaNm2aDh06pHfeeUcrVqwIeYkIAAB8dYX9EtK+ffs0fvx4e7stVBQXF6uyslJz5szR2bNn9dRTT6m5uVl33323PvjgAyUlJdnHvP3225o5c6buvfdexcbGqqioSK+88oq9PyUlRdXV1SopKdHo0aN16623av78+dxCDQAAJHUiwIwbN06WdeVbXWNiYrRo0SItWrToijV9+vTRmjVrrvp1hg8frl/84hfhLg8AAHwF8LeQAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYJz7aCwC6m9ue2xTtJVyRI87S0jHS0AVV8rfG2OOfLSmM4qoAIHxcgQEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMaJj/YCAETfbc9tivYSwvbZksJoLwFAFHEFBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcW7qAFNRUaHbbrtNSUlJGjt2rPbs2RPtJQEAgJvATXsb9TvvvKOysjKtXLlSY8eO1csvv6yCggIdOXJEaWlp0V4egCi7mW/9dsRZWjpGGrqgSv7WGHucW7+BrnPTXoF56aWX9OSTT+qxxx5Tdna2Vq5cqeTkZL355pvRXhoAAIiym/IKTEtLi+rq6jR37lx7LDY2Vnl5eaqtrW33GL/fL7/fb2+fOnVKknTy5EkFAoFOryUQCOjcuXP64osvlJCQIEmKv3i20/OhffFBS+fOBRUfiFVrMObaB6BT6HNkXKnPX3zxRRRX1Tljy2uivYSrcsRamjcyqBHPvyv//+/17rn3RnlV4aPPf3D69GlJkmVZVy+0bkKff/65JcnauXNnyPjs2bOtMWPGtHvM9773PUsSDx48ePDgwaMbPI4dO3bVrHBTXoHpjLlz56qsrMzeDgaDOnnypPr27auYmM7/n6bP59OAAQN07NgxOZ3Orlgq2kGfI4M+RwZ9jhx6HRmR7LNlWTp9+rQyMjKuWndTBphbb71VcXFxamxsDBlvbGxUenp6u8c4HA45HI6QsdTU1C5bk9Pp5JsjAuhzZNDnyKDPkUOvIyNSfU5JSblmzU35Jt7ExESNHj1aNTV/eE0wGAyqpqZGbrc7iisDAAA3g5vyCowklZWVqbi4WDk5ORozZoxefvllnT17Vo899li0lwYAAKLspg0w3/nOd/Q///M/mj9/vrxer0aMGKEPPvhALpcroutwOBz63ve+d9nLU+ha9Dky6HNk0OfIodeRcTP2OcayrnWfEgAAwM3lpnwPDAAAwNUQYAAAgHEIMAAAwDgEGAAAYBwCzFVUVFTotttuU1JSksaOHas9e/ZEe0lGKy8v1ze+8Q317t1baWlpevDBB3XkyJGQmgsXLqikpER9+/ZVr169VFRUdNkvNER4lixZopiYGJWWltpj9LnrfP755/rud7+rvn37qkePHho2bJj27dtn77csS/Pnz1e/fv3Uo0cP5eXl6ZNPPoniis3T2tqqF154QVlZWerRo4f+5E/+RIsXLw75Wzn0OXzbt2/X/fffr4yMDMXExOj9998P2d+Rnp48eVJTpkyR0+lUamqqpk2bpjNnzkTmBK7/Lxd1T2vXrrUSExOtN9980zp06JD15JNPWqmpqVZjY2O0l2asgoICa/Xq1dbBgwet+vp669vf/raVmZlpnTlzxq6ZPn26NWDAAKumpsbat2+flZuba33zm9+M4qrNtmfPHuu2226zhg8fbj3zzDP2OH3uGidPnrQGDhxo/c3f/I21e/du69NPP7Wqqqqs3/3ud3bNkiVLrJSUFOv999+3Pv74Y+sv/uIvrKysLOv8+fNRXLlZXnzxRatv377Wxo0braNHj1rr16+3evXqZa1YscKuoc/h27x5s/X8889b7777riXJeu+990L2d6Sn9913n3XnnXdau3btsn7xi19YgwYNsh555JGIrJ8AcwVjxoyxSkpK7O3W1lYrIyPDKi8vj+KqupempiZLkrVt2zbLsiyrubnZSkhIsNavX2/X/PrXv7YkWbW1tdFaprFOnz5tff3rX7c8Ho/1Z3/2Z3aAoc9d59lnn7XuvvvuK+4PBoNWenq69YMf/MAea25uthwOh/Vv//ZvkVhit1BYWGg9/vjjIWOTJ0+2pkyZYlkWfe4KXw4wHenp4cOHLUnW3r177Zqf//znVkxMjPX555/f8DXzElI7WlpaVFdXp7y8PHssNjZWeXl5qq2tjeLKupdTp05Jkvr06SNJqqurUyAQCOn74MGDlZmZSd87oaSkRIWFhSH9lOhzV/rZz36mnJwc/dVf/ZXS0tI0cuRI/fjHP7b3Hz16VF6vN6TXKSkpGjt2LL0Owze/+U3V1NTot7/9rSTp448/1o4dOzRp0iRJ9PlG6EhPa2trlZqaqpycHLsmLy9PsbGx2r179w1f4037m3ij6X//93/V2tp62W/9dblc+s1vfhOlVXUvwWBQpaWluuuuuzR06FBJktfrVWJi4mV/hNPlcsnr9UZhleZau3atfvnLX2rv3r2X7aPPXefTTz/V66+/rrKyMv3DP/yD9u7dq7/7u79TYmKiiouL7X6297OEXnfcc889J5/Pp8GDBysuLk6tra168cUXNWXKFEmizzdAR3rq9XqVlpYWsj8+Pl59+vSJSN8JMIiKkpISHTx4UDt27Ij2UrqdY8eO6ZlnnpHH41FSUlK0l9OtBYNB5eTk6B//8R8lSSNHjtTBgwe1cuVKFRcXR3l13ce6dev09ttva82aNbrjjjtUX1+v0tJSZWRk0OevMF5Casett96quLi4y+7KaGxsVHp6epRW1X3MnDlTGzdu1EcffaT+/fvb4+np6WppaVFzc3NIPX0PT11dnZqamjRq1CjFx8crPj5e27Zt0yuvvKL4+Hi5XC763EX69eun7OzskLEhQ4aooaFBkux+8rPk+syePVvPPfecHn74YQ0bNkxTp07VrFmzVF5eLok+3wgd6Wl6erqamppC9l+8eFEnT56MSN8JMO1ITEzU6NGjVVNTY48Fg0HV1NTI7XZHcWVmsyxLM2fO1HvvvactW7YoKysrZP/o0aOVkJAQ0vcjR46ooaGBvofh3nvv1YEDB1RfX28/cnJyNGXKFPtz+tw17rrrrst+FcBvf/tbDRw4UJKUlZWl9PT0kF77fD7t3r2bXofh3Llzio0N/ecqLi5OwWBQEn2+ETrSU7fbrebmZtXV1dk1W7ZsUTAY1NixY2/8Im/424QNtXbtWsvhcFiVlZXW4cOHraeeespKTU21vF5vtJdmrBkzZlgpKSnW1q1brRMnTtiPc+fO2TXTp0+3MjMzrS1btlj79u2z3G635Xa7o7jq7uHSu5Asiz53lT179ljx8fHWiy++aH3yySfW22+/bSUnJ1s/+clP7JolS5ZYqamp1k9/+lPrV7/6lfXAAw9we2+YiouLrT/6oz+yb6N+9913rVtvvdWaM2eOXUOfw3f69Glr//791v79+y1J1ksvvWTt37/f+q//+i/LsjrW0/vuu88aOXKktXv3bmvHjh3W17/+dW6jvhn88Ic/tDIzM63ExERrzJgx1q5du6K9JKNJavexevVqu+b8+fPW3/7t31q33HKLlZycbD300EPWiRMnorfobuLLAYY+d50NGzZYQ4cOtRwOhzV48GDrRz/6Ucj+YDBovfDCC5bL5bIcDod17733WkeOHInSas3k8/msZ555xsrMzLSSkpKsP/7jP7aef/55y+/32zX0OXwfffRRuz+Ti4uLLcvqWE+/+OIL65FHHrF69eplOZ1O67HHHrNOnz4dkfXHWNYlv8oQAADAALwHBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADj/D8eBLbv7h18GgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins = 10)\n",
    "# Based on the histogram we are selecting the max len as 50\n",
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# DataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "   def __init__(self, bert):      \n",
    "       super(BERT_Arch, self).__init__()\n",
    "       self.bert = bert \n",
    "      \n",
    "       # dropout layer\n",
    "       self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "       # relu activation function\n",
    "       self.relu =  nn.ReLU()\n",
    "       # dense layer\n",
    "       self.fc1 = nn.Linear(768,512)\n",
    "       self.fc2 = nn.Linear(512,256)\n",
    "       self.fc3 = nn.Linear(256,5)\n",
    "       #softmax activation function\n",
    "       self.softmax = nn.LogSoftmax(dim=1)\n",
    "       #define the forward pass\n",
    "   def forward(self, sent_id, mask):\n",
    "      #pass the inputs to the model  \n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      \n",
    "      x = self.fc2(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      # output layer\n",
    "      x = self.fc3(x)\n",
    "   \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BERT_Arch                                               --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              (23,440,896)\n",
       "│    │    └─Embedding: 3-2                              (393,216)\n",
       "│    │    └─LayerNorm: 3-3                              (1,536)\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
       "├─Dropout: 1-2                                          --\n",
       "├─ReLU: 1-3                                             --\n",
       "├─Linear: 1-4                                           393,728\n",
       "├─Linear: 1-5                                           131,328\n",
       "├─Linear: 1-6                                           1,285\n",
       "├─LogSoftmax: 1-7                                       --\n",
       "================================================================================\n",
       "Total params: 66,889,221\n",
       "Trainable params: 526,341\n",
       "Non-trainable params: 66,362,880\n",
       "================================================================================"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freeze all the parameters. This will prevent updating of model weights during fine-tuning.\n",
    "for param in bert.parameters():\n",
    "      param.requires_grad = False\n",
    "model = BERT_Arch(bert)\n",
    "# push the model to GPU\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.optimization because of the following error (look up to see its traceback):\nDescriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1031\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1031\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1032\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlr_scheduler\u001b[39;00m \u001b[39mimport\u001b[39;00m LambdaLR\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtrainer_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m SchedulerType\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer_utils.py:47\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseed_worker\u001b[39m(_):\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m attr_value_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m node_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_handle_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36\u001b[0m\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[0;32m     19\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m   serialized_pb\u001b[39m=\u001b[39m_b(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mtensorflow\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mz\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x10\u001b[39;00m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x14\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39munknown_rank\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x44\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39msize\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39mname\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mB\u001b[39m\u001b[39m\\x87\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x18\u001b[39;00m\u001b[39morg.tensorflow.frameworkB\u001b[39m\u001b[39m\\x11\u001b[39;00m\u001b[39mTensorShapeProtosP\u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[39m\\xf8\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x62\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[0;32m     30\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m   filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m   file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     34\u001b[0m   containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m   fields\u001b[39m=\u001b[39m[\n\u001b[1;32m---> 36\u001b[0m     _descriptor\u001b[39m.\u001b[39;49mFieldDescriptor(\n\u001b[0;32m     37\u001b[0m       name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m, full_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     38\u001b[0m       number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, cpp_type\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     39\u001b[0m       has_default_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, default_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     40\u001b[0m       message_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, enum_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, containing_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     41\u001b[0m       is_extension\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, extension_scope\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     42\u001b[0m       serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, file\u001b[39m=\u001b[39;49mDESCRIPTOR),\n\u001b[0;32m     43\u001b[0m     _descriptor\u001b[39m.\u001b[39mFieldDescriptor(\n\u001b[0;32m     44\u001b[0m       name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     45\u001b[0m       number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, cpp_type\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     46\u001b[0m       has_default_value\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, default_value\u001b[39m=\u001b[39m_b(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     47\u001b[0m       message_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enum_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m       is_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extension_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, file\u001b[39m=\u001b[39mDESCRIPTOR),\n\u001b[0;32m     50\u001b[0m   ],\n\u001b[0;32m     51\u001b[0m   extensions\u001b[39m=\u001b[39m[\n\u001b[0;32m     52\u001b[0m   ],\n\u001b[0;32m     53\u001b[0m   nested_types\u001b[39m=\u001b[39m[],\n\u001b[0;32m     54\u001b[0m   enum_types\u001b[39m=\u001b[39m[\n\u001b[0;32m     55\u001b[0m   ],\n\u001b[0;32m     56\u001b[0m   serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m   is_extendable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m   syntax\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m   extension_ranges\u001b[39m=\u001b[39m[],\n\u001b[0;32m     60\u001b[0m   oneofs\u001b[39m=\u001b[39m[\n\u001b[0;32m     61\u001b[0m   ],\n\u001b[0;32m     62\u001b[0m   serialized_start\u001b[39m=\u001b[39m\u001b[39m149\u001b[39m,\n\u001b[0;32m     63\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[0;32m     67\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     68\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[0;32m    101\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\protobuf\\descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, full_name, index, number, \u001b[39mtype\u001b[39m, cpp_type, label,\n\u001b[0;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[0;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    558\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    559\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, containing_oneof\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    560\u001b[0m             file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[0;32m    562\u001b[0m   \u001b[39mif\u001b[39;00m is_extension:\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\BOTSOMETHINGFROMNET\\main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BOTSOMETHINGFROMNET/main.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AdamW\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BOTSOMETHINGFROMNET/main.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# define the optimizer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BOTSOMETHINGFROMNET/main.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1021\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1020\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1021\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1022\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1023\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1033\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1032\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1033\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1034\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1035\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1036\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.optimization because of the following error (look up to see its traceback):\nDescriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35204251 2.47663551 1.45905024 2.02870813 1.73202614]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "print(class_wts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "# loss function\n",
    "cross_entropy = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "# number of training epochs\n",
    "epochs = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the gradients to 1.0 to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # clear calculated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in the form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate([p.detach().numpy() for p in total_preds], axis=0)\n",
    "\n",
    "    \n",
    "    # return the loss and predictions\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "print(f'\\nTraining Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Identified:  food_order.name.item\n",
      "Response: Thanks that will be $30\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load intents from JSON file\n",
    "with open('intents.json', 'r') as file:\n",
    "    intents = json.load(file)[\"intents\"]\n",
    "\n",
    "def get_prediction(str):\n",
    "    str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
    "    test_text = [str]\n",
    "    model.eval()\n",
    " \n",
    "    tokens_test_data = tokenizer(\n",
    "        test_text,\n",
    "        max_length=max_seq_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    preds = None\n",
    "    with torch.no_grad():\n",
    "        preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Intent Identified: \", le.inverse_transform(preds)[0])\n",
    "    return le.inverse_transform(preds)[0]\n",
    "\n",
    "def get_response(input_message):\n",
    "    intent_tag = get_prediction(input_message)\n",
    "    \n",
    "    # Find the intent in the intents list\n",
    "    matched_intent = next((i for i in intents if i[\"tag\"] == intent_tag), None)\n",
    "    \n",
    "    if matched_intent:\n",
    "        responses = matched_intent[\"responses\"]\n",
    "        result = random.choice(responses)\n",
    "        print(f\"Response: {result}\")\n",
    "       # return \"Intent: \" + intent_tag + '\\n' + \"Response: \" + result\n",
    "    else:\n",
    "        print(\"Intent not found in intents list\")\n",
    "        return \"Intent not found\"\n",
    "\n",
    "# Example usage\n",
    "get_response(\"Hi, I would like to order food for takeout for three people. I want to order sandwiches. I like to order Po' Boy sandwich, pastrami on rye sandwich, a french cheese steak sandwic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Assuming 'model' is your trained machine learning model\n",
    "with open('model1.pickle', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
